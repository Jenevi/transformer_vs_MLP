# Tiny Transformer Language Model

В репозитории теперь есть скрипт `tiny_transformer.py`, который обучает очень
маленькую трансформерную языковую модель на символьном уровне. Значения
гиперпараметров подобраны так, чтобы модель можно было запустить на CPU за
менее одной минуты, но вы можете менять их через параметры командной строки.

## Требования

- Python 3.10+
- [PyTorch](https://pytorch.org/) с CPU или GPU бэкендом

Удобно создать виртуальное окружение и установить `torch`:

```bash
python -m venv .venv
source .venv/bin/activate
pip install torch
```

## Запуск обучения

```bash
python tiny_transformer.py
```

По умолчанию скрипт использует встроенный небольшой корпус текста. Можно
передать собственный файл и настроить модель:

```bash
python tiny_transformer.py \
  --text-path data/my_text.txt \
  --batch-size 32 \
  --block-size 64 \
  --n-embd 64 \
  --n-head 4 \
  --n-layer 2 \
  --max-iters 1000
```

После обучения программа распечатает пример сгенерированного текста. Параметры
`--temperature` и `--sample-tokens` отвечают за разнообразие и длину
сэмплирования соответственно.

## Быстрый обзор структуры

- `tiny_transformer.py` — реализация токенизатора, архитектуры трансформера и
  цикла обучения.

Скрипт компактный и предназначен для изучения ключевых идей трансформеров.
