# Tiny Transformer Language Model

В репозитории теперь есть скрипт `tiny_transformer.py`, который обучает очень
маленькую трансформерную языковую модель на символьном уровне. Значения
гиперпараметров подобраны так, чтобы модель можно было запустить на CPU за
менее одной минуты, но вы можете менять их через параметры командной строки.

## Требования

- Python 3.10+
- [PyTorch](https://pytorch.org/) с CPU или GPU бэкендом

Удобно создать виртуальное окружение и установить `torch`:

```bash
python -m venv .venv
source .venv/bin/activate
pip install torch
```

## Запуск обучения

```bash
python tiny_transformer.py
```

По умолчанию скрипт использует встроенный небольшой корпус текста. Можно
передать собственный файл и настроить модель:

```bash
python tiny_transformer.py \
  --text-path data/my_text.txt \
  --batch-size 32 \
  --block-size 64 \
  --n-embd 64 \
  --n-head 4 \
  --n-layer 2 \
  --max-iters 1000
```

После обучения программа распечатает пример сгенерированного текста. Параметры
`--temperature` и `--sample-tokens` отвечают за разнообразие и длину
сэмплирования соответственно.

### Выбор архитектуры

По умолчанию запускается трансформер. Для сравнения можно обучить MLP-бейзлайн,
использующий те же данные и токенизатор:

```bash
python tiny_transformer.py --model mlp \
  --mlp-hidden 512 \
  --mlp-layers 3
```

Аргументы `--mlp-hidden` и `--mlp-layers` настраивают размер скрытого слоя и
число полносвязных блоков. Остальные параметры обучения общие для обеих
архитектур.

## Быстрый обзор структуры

- `tiny_transformer.py` — реализация токенизатора, трансформера и MLP-бейзлайна
  с общим циклом обучения и CLI.
- `tests/test_tiny_transformer.py` — базовые проверки токенизатора и обеих
  моделей.

Скрипт компактный и предназначен для изучения ключевых идей трансформеров и
сравнения с более простой архитектурой.
